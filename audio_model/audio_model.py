# audio_model.py

import os
import torch
import librosa
from transformers import AutoConfig, AutoFeatureExtractor, Wav2Vec2ForSequenceClassification

def load_audio_model(model_dir):
    """
    ì§€ì •í•œ ëª¨ë¸ ê²½ë¡œì—ì„œ êµ¬ì„±(config), extractor, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    config = AutoConfig.from_pretrained(model_dir, local_files_only=True)
    extractor = AutoFeatureExtractor.from_pretrained(model_dir, local_files_only=True)
    model = Wav2Vec2ForSequenceClassification.from_pretrained(
        model_dir, config=config, local_files_only=True
    )
    return model, extractor

def predict_audio(file_path, model_dir):
    """
    ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ë”¥í˜ì´í¬ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

    Args:
        file_path (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (.wav í˜•ì‹)
        model_dir (str): ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ

    Prints:
        ì˜ˆì¸¡ ê²°ê³¼ì™€ í™•ë¥  ì •ë³´
    """
    model, extractor = load_audio_model(model_dir)

    speech, _ = librosa.load(file_path, sr=16000)
    if len(speech) > 16000 * 15:
        speech = speech[:16000 * 15]

    inputs = extractor(speech, sampling_rate=16000, return_tensors="pt", padding=True)

    model.eval()
    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()

    real_prob, fake_prob = probs[0], probs[1]
    label = "UNSURE" if abs(real_prob - fake_prob) < 0.05 else ("REAL" if real_prob > fake_prob else "FAKE")

    print("ğŸ§ ì˜ˆì¸¡ ê²°ê³¼:", label)
    print(f"â†’ REAL í™•ë¥ : {real_prob:.4f}, FAKE í™•ë¥ : {fake_prob:.4f}")

# ì‚¬ìš© ì˜ˆì‹œ (ì§ì ‘ ì‹¤í–‰ ì‹œ)
if __name__ == "__main__":
    predict_audio("your_audio.wav", model_dir="audio_model_for_git")
